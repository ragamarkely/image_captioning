{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "The CNN-RNN architecture is as follows:\n",
    "* The CNN Encoder consists of the pre-trained ResNet50 and a trainable fully-connected layer with output size equal to the embedding size.\n",
    "* The RNN Decoder consists of an embedding layer, LSTM with dropout and a hidden layer, as well as a fully connected layer.\n",
    "\n",
    "The choices of variable values:\n",
    "* batch size: a few power of two values were tested (64 and 128). Batch size of 128 was selected because the batch could fit into memory, the size of the data is relatively large (there are 3,236 batches with this size), and the loss decreased over the number of steps.\n",
    "\n",
    "* vocab_threshold: several values were evaluated and threshold of 5 gave a reasonable number of tokens (8855).\n",
    "* embed_size: 512 was chosen following the size used in https://arxiv.org/pdf/1411.4555.pdf.\n",
    "* hidden_size: 512 was chosen following the size used in https://arxiv.org/pdf/1411.4555.pdf.\n",
    "* num_epochs: the training loss decreased and reached a plateau within 3 epochs, suggesting that this is sufficient for this project.\n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "The image trasformation was left as it is because it has already included important steps, such as resizing and cropping to ensure all images have the same size, data augmentation using horizontal flip to add some variation into the data, as well as normalization based on the input data used to pre-train the ResNet50 model. This transformation was sufficient to obtain a reasonable model performance.\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "The trainable parameters include embedding parameters of the CNN encoder and all parameters of the RNN decoder. The ResNet50 parameters of CNN encoder were obtained from the pre-training on ImageNet and not trained in order to avoid overfitting. The decoder parameters were all trainable because the total number of parameters are relatively reasonable (~9.2 millions), and in order to avoid overfitting, I used dropout for the LSTM model.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "I used Adam optimizer as it helps stabilize the parameter updates and accelerate the training using a combination of momentum and RMSprop approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.24s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 747/414113 [00:00<01:57, 3512.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:35<00:00, 4332.23it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.torch/models/resnet50-19c8e357.pth\n",
      "100%|██████████| 102502400/102502400 [00:00<00:00, 105665224.21it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 128          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(encoder.embed.parameters()) + list(decoder.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/3236], Loss: 3.6563, Perplexity: 38.7193\n",
      "Epoch [1/3], Step [200/3236], Loss: 3.2864, Perplexity: 26.74560\n",
      "Epoch [1/3], Step [300/3236], Loss: 3.2781, Perplexity: 26.5259\n",
      "Epoch [1/3], Step [400/3236], Loss: 3.3584, Perplexity: 28.7426\n",
      "Epoch [1/3], Step [500/3236], Loss: 2.8666, Perplexity: 17.5778\n",
      "Epoch [1/3], Step [600/3236], Loss: 2.9123, Perplexity: 18.3991\n",
      "Epoch [1/3], Step [700/3236], Loss: 3.4674, Perplexity: 32.0522\n",
      "Epoch [1/3], Step [800/3236], Loss: 2.5282, Perplexity: 12.5308\n",
      "Epoch [1/3], Step [900/3236], Loss: 2.6232, Perplexity: 13.7804\n",
      "Epoch [1/3], Step [1000/3236], Loss: 2.4696, Perplexity: 11.8176\n",
      "Epoch [1/3], Step [1100/3236], Loss: 2.9076, Perplexity: 18.3134\n",
      "Epoch [1/3], Step [1200/3236], Loss: 2.4896, Perplexity: 12.05595\n",
      "Epoch [1/3], Step [1300/3236], Loss: 2.8039, Perplexity: 16.5090\n",
      "Epoch [1/3], Step [1400/3236], Loss: 2.2983, Perplexity: 9.95711\n",
      "Epoch [1/3], Step [1500/3236], Loss: 2.4394, Perplexity: 11.4662\n",
      "Epoch [1/3], Step [1600/3236], Loss: 2.1741, Perplexity: 8.79456\n",
      "Epoch [1/3], Step [1700/3236], Loss: 2.3249, Perplexity: 10.2261\n",
      "Epoch [1/3], Step [1800/3236], Loss: 2.1369, Perplexity: 8.47322\n",
      "Epoch [1/3], Step [1900/3236], Loss: 2.1511, Perplexity: 8.59391\n",
      "Epoch [1/3], Step [2000/3236], Loss: 2.2558, Perplexity: 9.54294\n",
      "Epoch [1/3], Step [2100/3236], Loss: 2.3506, Perplexity: 10.4914\n",
      "Epoch [1/3], Step [2200/3236], Loss: 2.1164, Perplexity: 8.30136\n",
      "Epoch [1/3], Step [2300/3236], Loss: 2.2871, Perplexity: 9.84592\n",
      "Epoch [1/3], Step [2400/3236], Loss: 2.5159, Perplexity: 12.3773\n",
      "Epoch [1/3], Step [2500/3236], Loss: 2.2361, Perplexity: 9.35684\n",
      "Epoch [1/3], Step [2600/3236], Loss: 2.2781, Perplexity: 9.75775\n",
      "Epoch [1/3], Step [2700/3236], Loss: 2.1355, Perplexity: 8.46097\n",
      "Epoch [1/3], Step [2800/3236], Loss: 2.1138, Perplexity: 8.27961\n",
      "Epoch [1/3], Step [2900/3236], Loss: 2.2801, Perplexity: 9.77754\n",
      "Epoch [1/3], Step [3000/3236], Loss: 2.3114, Perplexity: 10.0885\n",
      "Epoch [1/3], Step [3100/3236], Loss: 2.1591, Perplexity: 8.66296\n",
      "Epoch [1/3], Step [3200/3236], Loss: 1.9975, Perplexity: 7.37059\n",
      "Epoch [2/3], Step [100/3236], Loss: 2.1933, Perplexity: 8.965103\n",
      "Epoch [2/3], Step [200/3236], Loss: 2.1100, Perplexity: 8.24865\n",
      "Epoch [2/3], Step [300/3236], Loss: 2.2023, Perplexity: 9.04548\n",
      "Epoch [2/3], Step [400/3236], Loss: 2.0620, Perplexity: 7.86204\n",
      "Epoch [2/3], Step [500/3236], Loss: 2.1272, Perplexity: 8.39129\n",
      "Epoch [2/3], Step [600/3236], Loss: 1.9413, Perplexity: 6.96771\n",
      "Epoch [2/3], Step [700/3236], Loss: 2.0027, Perplexity: 7.40874\n",
      "Epoch [2/3], Step [800/3236], Loss: 2.1162, Perplexity: 8.29922\n",
      "Epoch [2/3], Step [900/3236], Loss: 2.6652, Perplexity: 14.3712\n",
      "Epoch [2/3], Step [1000/3236], Loss: 1.9625, Perplexity: 7.1174\n",
      "Epoch [2/3], Step [1100/3236], Loss: 2.0169, Perplexity: 7.51524\n",
      "Epoch [2/3], Step [1200/3236], Loss: 2.0713, Perplexity: 7.93549\n",
      "Epoch [2/3], Step [1300/3236], Loss: 1.9488, Perplexity: 7.02052\n",
      "Epoch [2/3], Step [1400/3236], Loss: 2.0726, Perplexity: 7.94582\n",
      "Epoch [2/3], Step [1500/3236], Loss: 2.0450, Perplexity: 7.72918\n",
      "Epoch [2/3], Step [1600/3236], Loss: 2.1417, Perplexity: 8.51376\n",
      "Epoch [2/3], Step [1700/3236], Loss: 2.3052, Perplexity: 10.0257\n",
      "Epoch [2/3], Step [1800/3236], Loss: 2.4517, Perplexity: 11.6084\n",
      "Epoch [2/3], Step [1900/3236], Loss: 2.2005, Perplexity: 9.02971\n",
      "Epoch [2/3], Step [2000/3236], Loss: 2.0022, Perplexity: 7.40567\n",
      "Epoch [2/3], Step [2100/3236], Loss: 1.9765, Perplexity: 7.21761\n",
      "Epoch [2/3], Step [2200/3236], Loss: 2.0002, Perplexity: 7.39050\n",
      "Epoch [2/3], Step [2300/3236], Loss: 1.8950, Perplexity: 6.65243\n",
      "Epoch [2/3], Step [2400/3236], Loss: 2.2298, Perplexity: 9.29779\n",
      "Epoch [2/3], Step [2500/3236], Loss: 1.8200, Perplexity: 6.17184\n",
      "Epoch [2/3], Step [2600/3236], Loss: 2.0562, Perplexity: 7.81610\n",
      "Epoch [2/3], Step [2700/3236], Loss: 1.9925, Perplexity: 7.33411\n",
      "Epoch [2/3], Step [2800/3236], Loss: 1.9416, Perplexity: 6.97009\n",
      "Epoch [2/3], Step [2900/3236], Loss: 2.2294, Perplexity: 9.29391\n",
      "Epoch [2/3], Step [3000/3236], Loss: 1.9999, Perplexity: 7.38844\n",
      "Epoch [2/3], Step [3100/3236], Loss: 1.9788, Perplexity: 7.23415\n",
      "Epoch [2/3], Step [3200/3236], Loss: 2.0245, Perplexity: 7.57268\n",
      "Epoch [3/3], Step [100/3236], Loss: 2.2053, Perplexity: 9.072997\n",
      "Epoch [3/3], Step [200/3236], Loss: 1.8496, Perplexity: 6.35758\n",
      "Epoch [3/3], Step [300/3236], Loss: 2.1107, Perplexity: 8.25402\n",
      "Epoch [3/3], Step [400/3236], Loss: 2.1179, Perplexity: 8.31378\n",
      "Epoch [3/3], Step [500/3236], Loss: 1.9149, Perplexity: 6.78638\n",
      "Epoch [3/3], Step [600/3236], Loss: 2.1031, Perplexity: 8.19125\n",
      "Epoch [3/3], Step [700/3236], Loss: 2.4034, Perplexity: 11.0604\n",
      "Epoch [3/3], Step [800/3236], Loss: 2.0012, Perplexity: 7.39822\n",
      "Epoch [3/3], Step [900/3236], Loss: 2.5049, Perplexity: 12.2426\n",
      "Epoch [3/3], Step [1000/3236], Loss: 2.0305, Perplexity: 7.6179\n",
      "Epoch [3/3], Step [1100/3236], Loss: 1.8334, Perplexity: 6.25541\n",
      "Epoch [3/3], Step [1200/3236], Loss: 1.9491, Perplexity: 7.02262\n",
      "Epoch [3/3], Step [1300/3236], Loss: 1.9308, Perplexity: 6.89475\n",
      "Epoch [3/3], Step [1400/3236], Loss: 1.9463, Perplexity: 7.00281\n",
      "Epoch [3/3], Step [1500/3236], Loss: 2.1045, Perplexity: 8.20333\n",
      "Epoch [3/3], Step [1600/3236], Loss: 1.8984, Perplexity: 6.67529\n",
      "Epoch [3/3], Step [1700/3236], Loss: 1.9161, Perplexity: 6.79478\n",
      "Epoch [3/3], Step [1800/3236], Loss: 2.0716, Perplexity: 7.93786\n",
      "Epoch [3/3], Step [1900/3236], Loss: 1.8968, Perplexity: 6.66470\n",
      "Epoch [3/3], Step [2000/3236], Loss: 2.7703, Perplexity: 15.9641\n",
      "Epoch [3/3], Step [2100/3236], Loss: 2.0773, Perplexity: 7.98281\n",
      "Epoch [3/3], Step [2200/3236], Loss: 1.8010, Perplexity: 6.05566\n",
      "Epoch [3/3], Step [2300/3236], Loss: 1.9186, Perplexity: 6.81137\n",
      "Epoch [3/3], Step [2400/3236], Loss: 2.6322, Perplexity: 13.9045\n",
      "Epoch [3/3], Step [2500/3236], Loss: 1.8855, Perplexity: 6.58982\n",
      "Epoch [3/3], Step [2600/3236], Loss: 2.0314, Perplexity: 7.62456\n",
      "Epoch [3/3], Step [2700/3236], Loss: 1.8580, Perplexity: 6.41060\n",
      "Epoch [3/3], Step [2800/3236], Loss: 1.9437, Perplexity: 6.98487\n",
      "Epoch [3/3], Step [2900/3236], Loss: 1.8381, Perplexity: 6.28431\n",
      "Epoch [3/3], Step [3000/3236], Loss: 1.9981, Perplexity: 7.37506\n",
      "Epoch [3/3], Step [3100/3236], Loss: 1.8143, Perplexity: 6.13680\n",
      "Epoch [3/3], Step [3200/3236], Loss: 1.8185, Perplexity: 6.16268\n",
      "Epoch [3/3], Step [3236/3236], Loss: 2.2109, Perplexity: 9.12430"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# (Optional) TODO: Validate your model.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader_val import get_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_val = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize(\n",
    "        (0.485, 0.456, 0.406),                       # normalize image for pre-trained model\n",
    "        (0.229, 0.224, 0.225)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/202654 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202654/202654 [00:41<00:00, 4831.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the data loader.\n",
    "batch_size = 1\n",
    "vocab_threshold = 5\n",
    "val_data_loader = get_loader(\n",
    "    transform=transform_val,\n",
    "    mode=\"val\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embedding): Embedding(8855, 512)\n",
       "  (lstm): LSTM(512, 512, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=512, out_features=8855, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Load the trained CNN-RNN model.\n",
    "encoder_file = \"encoder-3.pkl\" \n",
    "decoder_file = \"decoder-3.pkl\"\n",
    "embed_size = 512\n",
    "hidden_size = 512\n",
    "vocab_size = len(val_data_loader.dataset.vocab)\n",
    "encoder = EncoderCNN(embed_size)\n",
    "encoder.eval()\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "decoder.eval()\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "\n",
    "# Move models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 202655.\n",
      "100 steps are done. Time taken so far = 6 sec.\n",
      "200 steps are done. Time taken so far = 12 sec.\n",
      "300 steps are done. Time taken so far = 18 sec.\n",
      "400 steps are done. Time taken so far = 23 sec.\n",
      "500 steps are done. Time taken so far = 29 sec.\n",
      "600 steps are done. Time taken so far = 35 sec.\n",
      "700 steps are done. Time taken so far = 41 sec.\n",
      "800 steps are done. Time taken so far = 47 sec.\n",
      "900 steps are done. Time taken so far = 53 sec.\n",
      "1000 steps are done. Time taken so far = 59 sec.\n",
      "1100 steps are done. Time taken so far = 64 sec.\n",
      "1200 steps are done. Time taken so far = 70 sec.\n",
      "1300 steps are done. Time taken so far = 76 sec.\n",
      "1400 steps are done. Time taken so far = 82 sec.\n",
      "1500 steps are done. Time taken so far = 88 sec.\n",
      "1600 steps are done. Time taken so far = 94 sec.\n",
      "1700 steps are done. Time taken so far = 100 sec.\n",
      "1800 steps are done. Time taken so far = 105 sec.\n",
      "1900 steps are done. Time taken so far = 111 sec.\n",
      "2000 steps are done. Time taken so far = 117 sec.\n",
      "2100 steps are done. Time taken so far = 123 sec.\n",
      "2200 steps are done. Time taken so far = 129 sec.\n",
      "2300 steps are done. Time taken so far = 135 sec.\n",
      "2400 steps are done. Time taken so far = 140 sec.\n",
      "2500 steps are done. Time taken so far = 146 sec.\n",
      "2600 steps are done. Time taken so far = 152 sec.\n",
      "2700 steps are done. Time taken so far = 158 sec.\n",
      "2800 steps are done. Time taken so far = 164 sec.\n",
      "2900 steps are done. Time taken so far = 170 sec.\n",
      "3000 steps are done. Time taken so far = 176 sec.\n",
      "3100 steps are done. Time taken so far = 182 sec.\n",
      "3200 steps are done. Time taken so far = 188 sec.\n",
      "3300 steps are done. Time taken so far = 194 sec.\n",
      "3400 steps are done. Time taken so far = 200 sec.\n",
      "3500 steps are done. Time taken so far = 206 sec.\n",
      "3600 steps are done. Time taken so far = 212 sec.\n",
      "3700 steps are done. Time taken so far = 218 sec.\n",
      "3800 steps are done. Time taken so far = 224 sec.\n",
      "3900 steps are done. Time taken so far = 230 sec.\n",
      "4000 steps are done. Time taken so far = 236 sec.\n",
      "4100 steps are done. Time taken so far = 242 sec.\n",
      "4200 steps are done. Time taken so far = 247 sec.\n",
      "4300 steps are done. Time taken so far = 253 sec.\n",
      "4400 steps are done. Time taken so far = 259 sec.\n",
      "4500 steps are done. Time taken so far = 265 sec.\n",
      "4600 steps are done. Time taken so far = 271 sec.\n",
      "4700 steps are done. Time taken so far = 277 sec.\n",
      "4800 steps are done. Time taken so far = 283 sec.\n",
      "4900 steps are done. Time taken so far = 289 sec.\n",
      "5000 steps are done. Time taken so far = 295 sec.\n",
      "5100 steps are done. Time taken so far = 301 sec.\n",
      "5200 steps are done. Time taken so far = 307 sec.\n",
      "5300 steps are done. Time taken so far = 313 sec.\n",
      "5400 steps are done. Time taken so far = 318 sec.\n",
      "5500 steps are done. Time taken so far = 324 sec.\n",
      "5600 steps are done. Time taken so far = 330 sec.\n",
      "5700 steps are done. Time taken so far = 336 sec.\n",
      "5800 steps are done. Time taken so far = 342 sec.\n",
      "5900 steps are done. Time taken so far = 348 sec.\n",
      "6000 steps are done. Time taken so far = 354 sec.\n",
      "6100 steps are done. Time taken so far = 360 sec.\n",
      "6200 steps are done. Time taken so far = 366 sec.\n",
      "6300 steps are done. Time taken so far = 372 sec.\n",
      "6400 steps are done. Time taken so far = 378 sec.\n",
      "6500 steps are done. Time taken so far = 384 sec.\n",
      "6600 steps are done. Time taken so far = 390 sec.\n",
      "6700 steps are done. Time taken so far = 396 sec.\n",
      "6800 steps are done. Time taken so far = 402 sec.\n",
      "6900 steps are done. Time taken so far = 408 sec.\n",
      "7000 steps are done. Time taken so far = 414 sec.\n",
      "7100 steps are done. Time taken so far = 420 sec.\n",
      "7200 steps are done. Time taken so far = 426 sec.\n",
      "7300 steps are done. Time taken so far = 432 sec.\n",
      "7400 steps are done. Time taken so far = 438 sec.\n",
      "7500 steps are done. Time taken so far = 444 sec.\n",
      "7600 steps are done. Time taken so far = 450 sec.\n",
      "7700 steps are done. Time taken so far = 456 sec.\n",
      "7800 steps are done. Time taken so far = 462 sec.\n",
      "7900 steps are done. Time taken so far = 468 sec.\n",
      "8000 steps are done. Time taken so far = 474 sec.\n",
      "8100 steps are done. Time taken so far = 480 sec.\n",
      "8200 steps are done. Time taken so far = 486 sec.\n",
      "8300 steps are done. Time taken so far = 492 sec.\n",
      "8400 steps are done. Time taken so far = 498 sec.\n",
      "8500 steps are done. Time taken so far = 504 sec.\n",
      "8600 steps are done. Time taken so far = 510 sec.\n",
      "8700 steps are done. Time taken so far = 516 sec.\n",
      "8800 steps are done. Time taken so far = 522 sec.\n",
      "8900 steps are done. Time taken so far = 528 sec.\n",
      "9000 steps are done. Time taken so far = 534 sec.\n",
      "9100 steps are done. Time taken so far = 540 sec.\n",
      "9200 steps are done. Time taken so far = 546 sec.\n",
      "9300 steps are done. Time taken so far = 552 sec.\n",
      "9400 steps are done. Time taken so far = 558 sec.\n",
      "9500 steps are done. Time taken so far = 564 sec.\n",
      "9600 steps are done. Time taken so far = 570 sec.\n",
      "9700 steps are done. Time taken so far = 576 sec.\n",
      "9800 steps are done. Time taken so far = 582 sec.\n",
      "9900 steps are done. Time taken so far = 588 sec.\n",
      "10000 steps are done. Time taken so far = 594 sec.\n",
      "10100 steps are done. Time taken so far = 600 sec.\n",
      "10200 steps are done. Time taken so far = 606 sec.\n",
      "10300 steps are done. Time taken so far = 612 sec.\n",
      "10400 steps are done. Time taken so far = 618 sec.\n",
      "10500 steps are done. Time taken so far = 624 sec.\n",
      "10600 steps are done. Time taken so far = 630 sec.\n",
      "10700 steps are done. Time taken so far = 636 sec.\n",
      "10800 steps are done. Time taken so far = 643 sec.\n",
      "10900 steps are done. Time taken so far = 649 sec.\n",
      "11000 steps are done. Time taken so far = 655 sec.\n",
      "11100 steps are done. Time taken so far = 661 sec.\n",
      "11200 steps are done. Time taken so far = 667 sec.\n",
      "11300 steps are done. Time taken so far = 673 sec.\n",
      "11400 steps are done. Time taken so far = 679 sec.\n",
      "11500 steps are done. Time taken so far = 685 sec.\n",
      "11600 steps are done. Time taken so far = 691 sec.\n",
      "11700 steps are done. Time taken so far = 697 sec.\n",
      "11800 steps are done. Time taken so far = 703 sec.\n",
      "11900 steps are done. Time taken so far = 709 sec.\n",
      "12000 steps are done. Time taken so far = 715 sec.\n",
      "12100 steps are done. Time taken so far = 720 sec.\n",
      "12200 steps are done. Time taken so far = 726 sec.\n",
      "12300 steps are done. Time taken so far = 732 sec.\n",
      "12400 steps are done. Time taken so far = 738 sec.\n",
      "12500 steps are done. Time taken so far = 744 sec.\n",
      "12600 steps are done. Time taken so far = 750 sec.\n",
      "12700 steps are done. Time taken so far = 756 sec.\n",
      "12800 steps are done. Time taken so far = 762 sec.\n",
      "12900 steps are done. Time taken so far = 768 sec.\n",
      "13000 steps are done. Time taken so far = 774 sec.\n",
      "13100 steps are done. Time taken so far = 780 sec.\n",
      "13200 steps are done. Time taken so far = 786 sec.\n",
      "13300 steps are done. Time taken so far = 792 sec.\n",
      "13400 steps are done. Time taken so far = 798 sec.\n",
      "13500 steps are done. Time taken so far = 804 sec.\n",
      "13600 steps are done. Time taken so far = 810 sec.\n",
      "13700 steps are done. Time taken so far = 816 sec.\n",
      "13800 steps are done. Time taken so far = 822 sec.\n",
      "13900 steps are done. Time taken so far = 828 sec.\n",
      "14000 steps are done. Time taken so far = 834 sec.\n",
      "14100 steps are done. Time taken so far = 840 sec.\n",
      "14200 steps are done. Time taken so far = 846 sec.\n",
      "14300 steps are done. Time taken so far = 852 sec.\n",
      "14400 steps are done. Time taken so far = 858 sec.\n",
      "14500 steps are done. Time taken so far = 864 sec.\n",
      "14600 steps are done. Time taken so far = 870 sec.\n",
      "14700 steps are done. Time taken so far = 876 sec.\n",
      "14800 steps are done. Time taken so far = 882 sec.\n",
      "14900 steps are done. Time taken so far = 888 sec.\n",
      "15000 steps are done. Time taken so far = 894 sec.\n",
      "15100 steps are done. Time taken so far = 900 sec.\n",
      "15200 steps are done. Time taken so far = 906 sec.\n",
      "15300 steps are done. Time taken so far = 912 sec.\n",
      "15400 steps are done. Time taken so far = 918 sec.\n",
      "15500 steps are done. Time taken so far = 924 sec.\n",
      "15600 steps are done. Time taken so far = 930 sec.\n",
      "15700 steps are done. Time taken so far = 936 sec.\n",
      "15800 steps are done. Time taken so far = 942 sec.\n",
      "15900 steps are done. Time taken so far = 948 sec.\n",
      "16000 steps are done. Time taken so far = 954 sec.\n",
      "16100 steps are done. Time taken so far = 960 sec.\n",
      "16200 steps are done. Time taken so far = 966 sec.\n",
      "16300 steps are done. Time taken so far = 972 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16400 steps are done. Time taken so far = 977 sec.\n",
      "16500 steps are done. Time taken so far = 983 sec.\n",
      "16600 steps are done. Time taken so far = 989 sec.\n",
      "16700 steps are done. Time taken so far = 995 sec.\n",
      "16800 steps are done. Time taken so far = 1001 sec.\n",
      "16900 steps are done. Time taken so far = 1007 sec.\n",
      "17000 steps are done. Time taken so far = 1013 sec.\n",
      "17100 steps are done. Time taken so far = 1019 sec.\n",
      "17200 steps are done. Time taken so far = 1025 sec.\n",
      "17300 steps are done. Time taken so far = 1031 sec.\n",
      "17400 steps are done. Time taken so far = 1037 sec.\n",
      "17500 steps are done. Time taken so far = 1043 sec.\n",
      "17600 steps are done. Time taken so far = 1049 sec.\n",
      "17700 steps are done. Time taken so far = 1055 sec.\n",
      "17800 steps are done. Time taken so far = 1061 sec.\n",
      "17900 steps are done. Time taken so far = 1067 sec.\n",
      "18000 steps are done. Time taken so far = 1073 sec.\n",
      "18100 steps are done. Time taken so far = 1079 sec.\n",
      "18200 steps are done. Time taken so far = 1085 sec.\n",
      "18300 steps are done. Time taken so far = 1091 sec.\n",
      "18400 steps are done. Time taken so far = 1097 sec.\n",
      "18500 steps are done. Time taken so far = 1103 sec.\n",
      "18600 steps are done. Time taken so far = 1109 sec.\n",
      "18700 steps are done. Time taken so far = 1115 sec.\n",
      "18800 steps are done. Time taken so far = 1121 sec.\n",
      "18900 steps are done. Time taken so far = 1127 sec.\n",
      "19000 steps are done. Time taken so far = 1133 sec.\n",
      "19100 steps are done. Time taken so far = 1139 sec.\n",
      "19200 steps are done. Time taken so far = 1145 sec.\n",
      "19300 steps are done. Time taken so far = 1151 sec.\n",
      "19400 steps are done. Time taken so far = 1157 sec.\n",
      "19500 steps are done. Time taken so far = 1163 sec.\n",
      "19600 steps are done. Time taken so far = 1169 sec.\n",
      "19700 steps are done. Time taken so far = 1175 sec.\n",
      "19800 steps are done. Time taken so far = 1181 sec.\n",
      "19900 steps are done. Time taken so far = 1187 sec.\n",
      "20000 steps are done. Time taken so far = 1193 sec.\n",
      "20100 steps are done. Time taken so far = 1199 sec.\n",
      "20200 steps are done. Time taken so far = 1205 sec.\n",
      "20300 steps are done. Time taken so far = 1211 sec.\n",
      "20400 steps are done. Time taken so far = 1217 sec.\n",
      "20500 steps are done. Time taken so far = 1223 sec.\n",
      "20600 steps are done. Time taken so far = 1229 sec.\n",
      "20700 steps are done. Time taken so far = 1235 sec.\n",
      "20800 steps are done. Time taken so far = 1241 sec.\n",
      "20900 steps are done. Time taken so far = 1247 sec.\n",
      "21000 steps are done. Time taken so far = 1253 sec.\n",
      "21100 steps are done. Time taken so far = 1259 sec.\n",
      "21200 steps are done. Time taken so far = 1265 sec.\n",
      "21300 steps are done. Time taken so far = 1271 sec.\n",
      "21400 steps are done. Time taken so far = 1277 sec.\n",
      "21500 steps are done. Time taken so far = 1283 sec.\n",
      "21600 steps are done. Time taken so far = 1289 sec.\n",
      "21700 steps are done. Time taken so far = 1295 sec.\n",
      "21800 steps are done. Time taken so far = 1301 sec.\n",
      "21900 steps are done. Time taken so far = 1307 sec.\n",
      "22000 steps are done. Time taken so far = 1313 sec.\n",
      "22100 steps are done. Time taken so far = 1319 sec.\n",
      "22200 steps are done. Time taken so far = 1325 sec.\n",
      "22300 steps are done. Time taken so far = 1331 sec.\n",
      "22400 steps are done. Time taken so far = 1337 sec.\n",
      "22500 steps are done. Time taken so far = 1343 sec.\n",
      "22600 steps are done. Time taken so far = 1349 sec.\n",
      "22700 steps are done. Time taken so far = 1355 sec.\n",
      "22800 steps are done. Time taken so far = 1361 sec.\n",
      "22900 steps are done. Time taken so far = 1367 sec.\n",
      "23000 steps are done. Time taken so far = 1373 sec.\n",
      "23100 steps are done. Time taken so far = 1379 sec.\n",
      "23200 steps are done. Time taken so far = 1385 sec.\n",
      "23300 steps are done. Time taken so far = 1391 sec.\n",
      "23400 steps are done. Time taken so far = 1397 sec.\n",
      "23500 steps are done. Time taken so far = 1403 sec.\n",
      "23600 steps are done. Time taken so far = 1409 sec.\n",
      "23700 steps are done. Time taken so far = 1415 sec.\n",
      "23800 steps are done. Time taken so far = 1421 sec.\n",
      "23900 steps are done. Time taken so far = 1427 sec.\n",
      "24000 steps are done. Time taken so far = 1433 sec.\n",
      "24100 steps are done. Time taken so far = 1439 sec.\n",
      "24200 steps are done. Time taken so far = 1445 sec.\n",
      "24300 steps are done. Time taken so far = 1451 sec.\n",
      "24400 steps are done. Time taken so far = 1456 sec.\n",
      "24500 steps are done. Time taken so far = 1462 sec.\n",
      "24600 steps are done. Time taken so far = 1468 sec.\n",
      "24700 steps are done. Time taken so far = 1474 sec.\n",
      "24800 steps are done. Time taken so far = 1480 sec.\n",
      "24900 steps are done. Time taken so far = 1486 sec.\n",
      "25000 steps are done. Time taken so far = 1492 sec.\n",
      "25100 steps are done. Time taken so far = 1498 sec.\n",
      "25200 steps are done. Time taken so far = 1504 sec.\n",
      "25300 steps are done. Time taken so far = 1510 sec.\n",
      "25400 steps are done. Time taken so far = 1516 sec.\n",
      "25500 steps are done. Time taken so far = 1522 sec.\n",
      "25600 steps are done. Time taken so far = 1528 sec.\n",
      "25700 steps are done. Time taken so far = 1534 sec.\n",
      "25800 steps are done. Time taken so far = 1540 sec.\n",
      "25900 steps are done. Time taken so far = 1546 sec.\n",
      "26000 steps are done. Time taken so far = 1552 sec.\n",
      "26100 steps are done. Time taken so far = 1558 sec.\n",
      "26200 steps are done. Time taken so far = 1564 sec.\n",
      "26300 steps are done. Time taken so far = 1570 sec.\n",
      "26400 steps are done. Time taken so far = 1575 sec.\n",
      "26500 steps are done. Time taken so far = 1581 sec.\n",
      "26600 steps are done. Time taken so far = 1587 sec.\n",
      "26700 steps are done. Time taken so far = 1593 sec.\n",
      "26800 steps are done. Time taken so far = 1599 sec.\n",
      "26900 steps are done. Time taken so far = 1605 sec.\n",
      "27000 steps are done. Time taken so far = 1611 sec.\n",
      "27100 steps are done. Time taken so far = 1617 sec.\n",
      "27200 steps are done. Time taken so far = 1623 sec.\n",
      "27300 steps are done. Time taken so far = 1629 sec.\n",
      "27400 steps are done. Time taken so far = 1635 sec.\n",
      "27500 steps are done. Time taken so far = 1641 sec.\n",
      "27600 steps are done. Time taken so far = 1647 sec.\n",
      "27700 steps are done. Time taken so far = 1653 sec.\n",
      "27800 steps are done. Time taken so far = 1659 sec.\n",
      "27900 steps are done. Time taken so far = 1665 sec.\n",
      "28000 steps are done. Time taken so far = 1671 sec.\n",
      "28100 steps are done. Time taken so far = 1677 sec.\n",
      "28200 steps are done. Time taken so far = 1683 sec.\n",
      "28300 steps are done. Time taken so far = 1689 sec.\n",
      "28400 steps are done. Time taken so far = 1695 sec.\n",
      "28500 steps are done. Time taken so far = 1701 sec.\n",
      "28600 steps are done. Time taken so far = 1707 sec.\n",
      "28700 steps are done. Time taken so far = 1713 sec.\n",
      "28800 steps are done. Time taken so far = 1719 sec.\n",
      "28900 steps are done. Time taken so far = 1725 sec.\n",
      "29000 steps are done. Time taken so far = 1731 sec.\n",
      "29100 steps are done. Time taken so far = 1737 sec.\n",
      "29200 steps are done. Time taken so far = 1742 sec.\n",
      "29300 steps are done. Time taken so far = 1748 sec.\n",
      "29400 steps are done. Time taken so far = 1754 sec.\n",
      "29500 steps are done. Time taken so far = 1760 sec.\n",
      "29600 steps are done. Time taken so far = 1766 sec.\n",
      "29700 steps are done. Time taken so far = 1772 sec.\n",
      "29800 steps are done. Time taken so far = 1778 sec.\n",
      "29900 steps are done. Time taken so far = 1784 sec.\n",
      "30000 steps are done. Time taken so far = 1790 sec.\n"
     ]
    }
   ],
   "source": [
    "import json, time, math\n",
    "\n",
    "annotation_file = \"/opt/cocoapi/annotations/captions_val2014.json\"\n",
    "results_file = \"captions_val2014_results.json\"\n",
    "\n",
    "def clean_candidate(output):\n",
    "    res = []\n",
    "    for word_ix in output[1: -1]:\n",
    "        word = val_data_loader.dataset.vocab.idx2word[word_ix]\n",
    "        if word != \".\":\n",
    "            res.append(word)\n",
    "    return res\n",
    "\n",
    "def clean_reference(output):\n",
    "    res = []\n",
    "    for cap in output:\n",
    "        lst = []\n",
    "        \n",
    "        for word_ix in cap[1: -1]:\n",
    "            word = val_data_loader.dataset.vocab.idx2word[word_ix]\n",
    "            if word != \".\":\n",
    "                lst.append(word)\n",
    "        res.append(lst)\n",
    "    return res\n",
    "\n",
    "total_steps = math.ceil(len(val_data_loader.dataset.caption_lengths) / val_data_loader.batch_sampler.batch_size)\n",
    "print(f\"Total steps: {total_steps + 1}.\")\n",
    "candidate_caption_list = []\n",
    "reference_caption_list = []\n",
    "start = time.time()\n",
    "n = 30000\n",
    "for i in range(n):\n",
    "    # Obtain the batch.\n",
    "    image_id, image, ref_caption = next(iter(val_data_loader))\n",
    "\n",
    "    # Move batch of images and captions to GPU if CUDA is available.\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Get predicted captions.\n",
    "    features = encoder(image).unsqueeze(1)\n",
    "    \n",
    "    output = decoder.sample(features)     \n",
    "    candidate_caption_list.append(clean_candidate(output))\n",
    "    reference_caption_list.append(clean_reference(ref_caption.tolist()))\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"{i + 1} steps are done. Time taken so far = {round(time.time() - start)} sec.\")\n",
    "        \n",
    "# with open(results_file, 'w') as fout:\n",
    "#     json.dump(output_list, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: failed\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - torchtext\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/conda-forge/linux-64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://repo.anaconda.com/pkgs/main/linux-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/free/linux-64\n",
      "  - https://repo.anaconda.com/pkgs/free/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/linux-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu-1 score: 0.33215182538856725\n",
      "Bleu-2 score: 0.18461409620256236\n",
      "Bleu-3 score: 0.10585420139024151\n",
      "Bleu-4 score: 0.06374936226471205\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "for i in range(1, 5):\n",
    "    score = bleu_score(\n",
    "        candidate_corpus=candidate_caption_list, \n",
    "        references_corpus=reference_caption_list, \n",
    "        max_n=i,\n",
    "        weights=[1/i for _ in range(i)]\n",
    "    )\n",
    "    print(f\"Bleu-{i} score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
